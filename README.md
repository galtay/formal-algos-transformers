# formal-algos-transformers

Implements the algorithms in
Formal Algorithms for Transformers
https://arxiv.org/abs/2207.09238

BUT

* tensors are transposed
* batch dimension is included
* masking due to padding is included

Inspiration

* https://nlp.seas.harvard.edu/2018/04/03/attention.html
* http://nlp.seas.harvard.edu/annotated-transformer/
* https://github.com/labmlai/annotated_deep_learning_paper_implementations
